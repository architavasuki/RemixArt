{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "sys.path.append('.')\n",
    "# nltk.download('stopwords')\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{\"'s\": <gensim.models.keyedvectors.Vocab object at 0x7f22da5f8d50>, 'buying': <gensim.models.keyedvectors.Vocab object at 0x7f22da5f8d90>, 'stairway': <gensim.models.keyedvectors.Vocab object at 0x7f22da5f8090>, 'heaven': <gensim.models.keyedvectors.Vocab object at 0x7f22da793cd0>, 'help': <gensim.models.keyedvectors.Vocab object at 0x7f22da5c2590>, 'need': <gensim.models.keyedvectors.Vocab object at 0x7f22da621c50>, 'somebody': <gensim.models.keyedvectors.Vocab object at 0x7f22da6215d0>, 'anybody': <gensim.models.keyedvectors.Vocab object at 0x7f22da621dd0>, 'someone': <gensim.models.keyedvectors.Vocab object at 0x7f22da7938d0>, 'helppppp': <gensim.models.keyedvectors.Vocab object at 0x7f22da5c2e90>}\n[-2.0143199e-03 -4.4827542e-04 -1.9989237e-03  1.3504165e-03\n -1.4786172e-03 -2.8101471e-03  4.0211435e-03  3.8053938e-03\n -8.4860559e-04  4.4311318e-03  4.4809170e-03  4.3492666e-03\n  7.9447532e-04 -5.0487078e-04  4.6522631e-03 -5.8973208e-04\n -3.9836671e-04 -3.1705368e-03  4.8308964e-03  5.3634180e-04\n -2.1368754e-03  1.1897654e-03  3.5823837e-03 -2.8247712e-03\n  6.9332024e-04 -4.1480032e-03  2.2337162e-03 -4.4626915e-03\n -3.7468580e-04  1.6317458e-03 -2.9390466e-03  4.8869955e-03\n  3.9606616e-03 -3.3959448e-03  1.7696673e-03  2.3237814e-03\n  1.4920934e-03  1.6342486e-03  3.1253375e-04 -3.2773984e-03\n -8.5636252e-04  4.7733393e-03  2.3228894e-03 -7.2367355e-04\n -2.9310428e-03 -4.6458668e-03  1.4963134e-03 -3.2500033e-03\n  1.9017524e-03 -1.4902577e-03 -2.1235552e-03 -2.5079029e-03\n  3.1939391e-03  3.3820062e-03 -4.5042527e-03 -6.7405961e-04\n  4.0510846e-03 -4.1303923e-04  3.2469884e-03  1.6125707e-03\n -1.7876446e-03 -4.2777555e-03  2.8044572e-03  4.5816526e-03\n -3.0383032e-03 -4.9265339e-03  2.3719929e-03  2.1334523e-03\n  3.7759473e-03  7.0583518e-04  2.0379608e-03 -1.5787525e-03\n -4.3770717e-03 -4.7986419e-03  3.5370805e-03  3.6344945e-03\n  1.8739095e-03  3.4029584e-03 -3.8993636e-03 -2.6478423e-03\n  4.0592840e-03 -2.0816989e-04  3.7267867e-03  3.1844252e-03\n -4.9705645e-03  2.8374500e-03  3.2132044e-03 -3.0925657e-05\n  5.3659442e-04  2.3833010e-03 -2.6361507e-03 -7.4044726e-04\n -2.2956273e-03  4.4607245e-03 -1.6256305e-03 -1.4229766e-03\n  2.5683427e-03  2.3483839e-03 -6.1107666e-04 -3.6397961e-03]\n"
    }
   ],
   "source": [
    "from gensim.models import word2vec as w2v\n",
    "\n",
    "sent = [[\"'s\", 'buying', 'stairway', 'heaven'], ['help', 'need', 'somebody', 'help', 'anybody', 'help', 'need', 'someone', 'helppppp']]\n",
    "\n",
    "model = w2v.Word2Vec(min_count=1)\n",
    "model.build_vocab(sent)\n",
    "print(model.wv.vocab)\n",
    "model.train(sent,total_examples=len(sent),epochs=1)\n",
    "print (model.wv['buying'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fb33ba6c2738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m params = {\"batch_size\": args.batch_size,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0;34m\"shuffle\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \"num_workers\": args.workers,}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# params = {\"batch_size\": args.batch_size,\n",
    "#                        \"shuffle\": True,\n",
    "#                        \"num_workers\": args.workers,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    sw = set(list(stopwords.words('english'))+ re.split('',\"!\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"))\n",
    "    text = text.lower()\n",
    "    word_token = nltk.word_tokenize(text)\n",
    "    word_token = [word for word in word_token if word not in sw]\n",
    "    return word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for text in (os.listdir(path)):\n",
    "        with open(os.path.join(path,text),'r') as file:\n",
    "            texts.append(\n",
    "                preprocess(str(file.read()))\n",
    "            )\n",
    "            labels.append(text[:-4])\n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel():\n",
    "\n",
    "    def __init__(self,args):\n",
    "        '''\n",
    "            A wrapper class based on Gensim Word2Vec model.\n",
    "            Required Libraries: re, gensim, nltk\n",
    "        '''\n",
    "        self.built = False\n",
    "        self.args = args\n",
    "        self.trained = False\n",
    "        self.data = False\n",
    "        self.sw = set(list(stopwords.words('english'))+ re.split('',\"!\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"))\n",
    "        print('embedding model initialized with parameters:\\n', args)\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "            Builds a new model with parameters specified in args.\n",
    "        '''\n",
    "        if not self.built and self.data:\n",
    "            self.model = Word2Vec(\n",
    "                            self.texts,\n",
    "                            size = self.args['size'], \n",
    "                            min_count = self.args['min_count'],\n",
    "                            workers = self.args['workers'],\n",
    "                            sg = self.args['sg'],\n",
    "                        )\n",
    "            self.build = True\n",
    "        else:\n",
    "            print('model alreay built')\n",
    "\n",
    "    def load_model(self,path):\n",
    "        '''\n",
    "            Loads the existing model from specified path\n",
    "        '''\n",
    "        if self.build:\n",
    "            print('A model exists. Still want to load?(y or n')\n",
    "            if str(input()) == 'y':\n",
    "                self.model = Word2Vec.load(path)\n",
    "                self.built = True\n",
    "\n",
    "    def save_model(self,path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_data(self,path):\n",
    "        texts = []\n",
    "        labels = []\n",
    "        print(f'loading data from {path}...')\n",
    "        for text in (os.listdir(path)):\n",
    "            with open(os.path.join(path,text),'r') as file:\n",
    "                texts.append(\n",
    "                    self.preprocess(str(file.read()))\n",
    "                )\n",
    "                labels.append(text.split('.')[0])\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.data = True\n",
    "        print(f'Total {len(texts)} texts loaded...')\n",
    "\n",
    "    def __train(self,text,epochs=5):\n",
    "        if self.data:\n",
    "            self.text = text\n",
    "            self.model.train(\n",
    "                self.text, \n",
    "                total_examples=len(self.texts), \n",
    "                epochs=epochs\n",
    "            )\n",
    "        else:\n",
    "            print('Load the data first.')\n",
    "\n",
    "    def embedd(self,sentence):\n",
    "        if self.build:\n",
    "            return self.model.wv[self.preprocess(sentence)]\n",
    "        else: print(\"Build the model first.\")\n",
    "\n",
    "    def save_embeddings(self,path):\n",
    "        embeddings = list(map(self.embedd,[' '.join(x) for x in self.texts]))\n",
    "        print(f'saving embeddings to {path}')\n",
    "        with open(path,'wb') as file:\n",
    "            pickle.dump(np.array(embeddings),file)\n",
    "        return embeddings\n",
    "    \n",
    "    def preprocess(self,text):\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        word_token = nltk.word_tokenize(text)\n",
    "        word_token = [word for word in word_token if word not in self.sw]\n",
    "        return word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'size' : 100, \n",
    "    'min_count' : 1,\n",
    "    'workers' : os.cpu_count(),\n",
    "    \"sg\" : 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "embedding model initialized with parameters:\n {'size': 100, 'min_count': 1, 'workers': 8, 'sg': 1}\nloading data from training...\nTotal 2 texts loaded...\n"
    }
   ],
   "source": [
    "em = EmbeddingModel(args)\n",
    "em.load_data('training')\n",
    "em.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, args):\n",
    "        self.vocab = args.vocab\n",
    "        self.max_len = args.max_length\n",
    "\n",
    "        self.length = len(texts)\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.identity = np.eye(len(self.vocab))\n",
    "        self.stopwords = set(list(stopwords.words('english'))+ re.split('',\"!\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def preprocess(self,text):\n",
    "        text = text.lower()\n",
    "        word_token = nltk.word_tokenize(text)\n",
    "        word_token = [word for word in word_token if word not in self.stopwords]\n",
    "        return word_token\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        raw_text = self.texts[index]\n",
    "        processed_text = self.preprocess(raw_text)\n",
    "\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return ' '.join(processed_text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(load_data('training')[0], [1,2],args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('help need somebody help anybody help need someone helppppp',\n  \"'s buying stairway heaven\"),\n tensor([2, 1])]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "X = iter(dataloader)\n",
    "X = X.next()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m,mean=0.0,var=0.05):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(mean, var)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharCNN,self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "                        nn.Conv1d(len(args.vocab), 256, kernel_size=7, padding=0),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(3),\n",
    "                        nn.Conv1d(256, 256, kernel_size=7, padding=0),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(3),\n",
    "                        nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(3),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(256*34,1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.5),\n",
    "                        nn.Linear(1024, 1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CharCNN(\n  (model): Sequential(\n    (0): Conv1d(70, 256, kernel_size=(7,), stride=(1,))\n    (1): ReLU()\n    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n    (4): ReLU()\n    (5): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n    (7): ReLU()\n    (8): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n    (9): ReLU()\n    (10): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n    (11): ReLU()\n    (12): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n    (13): ReLU()\n    (14): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (15): Flatten()\n    (16): Linear(in_features=8704, out_features=1024, bias=True)\n    (17): ReLU()\n    (18): Dropout(p=0.5, inplace=False)\n    (19): Linear(in_features=1024, out_features=1024, bias=True)\n    (20): ReLU()\n    (21): Dropout(p=0.5, inplace=False)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "net = CharCNN()\n",
    "net.apply(weights_init_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iter(dataloader)\n",
    "X = X.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 1024])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "net(X[0].view(-1,70,1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitremixartconda754be6cf19a94920834a6bc6331bcd85",
   "display_name": "Python 3.7.7 64-bit ('RemixArt': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}